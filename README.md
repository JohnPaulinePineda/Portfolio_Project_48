# [Supervised Learning : Exploring Activation Functions And Backpropagation Gradient Updates for Neural Network Classification](https://johnpaulinepineda.github.io/Portfolio_Project_48/)

[![](https://img.shields.io/badge/Python-black?logo=Python)](#) [![](https://img.shields.io/badge/Jupyter-black?logo=Jupyter)](#)

This [project](https://johnpaulinepineda.github.io/Portfolio_Project_48/) manually implements the Sigmoid, Rectified Linear Unit, Leaky Rectified Linear Unit, Exponential Linear Unit, Scaled Exponential Linear Unit and Randomized Leaky Rectified Linear Unit activation functions with fixed values applied for the learning rate and iteration count parameters to optimally update the gradients and weights of an artificial neural network classification model. The gradient, weight, cost function and classification accuracy optimization profiles of the different activation settings were evaluated and compared.

<img src="docs/Project48_Summary.png?raw=true"/>
